\chapter{Experimental reinforcement learning setup}
\label{chap:setup}

Details of the components and techniques required for optimizing order placement was provided in Chapter \ref{chap:preliminaries}, and previous approaches pursued by other researchers were introduced in Chapter \ref{chap:related-work}.
The process of collecting historical event data and the construction of a limit order book was explained in Chapter \ref{chap:data}.
The data was investigated and features were constructed to be used within the reinforcement learning setup.
The next step is to build a reinforcement learning environment which is flexible enough to enable a) investigations with various types of features and agents to proceed, and b) adjustments to be made to important environment parameters.
The correctness of this environment is critical as it emulates a stock exchange and therefore determines how orders would have been transacted in the past.
If the implementation varies from the one used in exchanges, or does not cover certain edge cases, the matching of placed orders would differ significantly from the one in a production setup.

Therefore, this chapter aims to build an environment that includes the desired capabilities of a real world exchange in order to determine how limit orders would have been processed, had they been placed at a given point in time in the past.
First, the setup of the environment is described, whereby we explain how the components involved work in combination such that a learner can simulate order placement.
Finally, two implementations of reinforcement learning agents are provided.
A Q-Learning agent will serve as the learner when no market variables are provided and a Deep Q-Network agent is developed to handle the features previously developed.

\section{Order Placement Environment}
\label{sec:setup-order-placement}

The reinforcement learning environment, that emulates order placement on historical market data, is introduced in this section.
This environment enables an agent to buy or sell $V$ shares within a time horizon $H$ and makes extensive use of the components previously described in Chapter \ref{chap:preliminaries}.
It works principally by an agent observing a state $s_t$ (observation state) at some time $t$ and responding with an action $a_t$ that indicates the price at which to place the order in the order book.
The task of the environment is then to evaluate the outcome of the order placed and report to the agent along with a reward $r_{t+1}$ and the next state $s_{t+1}$.
Subsequently, the order is canceled so that the agent can submit a new action for the remaining shares to be bought or sold.

OpenAI Gym \cite{brockman2016openai} is an open source toolkit for reinforcement learning.
The interfaces of this toolkit were used in order to follow their standards while building this environment.
The advantage of this is that any OpenAI Gym compatible agent and bench-marking tools can be applied in this environment.

\subsection{Overview of components}

\begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=14cm]{images/rl-env-overview}
    }
    \caption{Overview of reinforcement learning order placement environment.}
    \label{fig:rl-env-overview}
\end{figure}

Figure \ref{fig:rl-env-overview} shows the inner workings of the order placement environment.
An agent will simulate and complete the placement of one order of $V$ shares with time horizon $H$ within one \textit{epoch}.
More precisely, the agent initializes an epoch by using the \texttt{reset} function, which clears the internal state of the environment stored in its memory.
The internal state consists of the remaining shares the agent has to buy or sell (as denoted by the \textit{inventory} $i$), the time $t$ that the agent has left for the epoch, and the ongoing \textit{order} that is to be placed.
In addition, a random point in time in the historical data set is chosen for the agent to run the initiated epoch (e.g. placement of the order).
The agent explores the environment using the $step$ function which it uses to send the desired action in order to place the order at a certain price level.
The first component of the environment to react to the action when it is received is the \textit{order handling component}. This recalls the order the agent is currently trying to fill and adjusts the price in response to the action received.
Subsequently, the order is forwarded to the \textit{match engine}, which attempts to execute the order within the historical order book.
The order, as well as the possible resulting trades generated during the matching process are then forwarded to the \textit{evaluation component}.
Since it can take multiple steps for the agent to fill an order, this component is responsible for updating and storing the remaining inventory and the consumed time horizon of the order in the \textit{memory}.
Additionally, the index of the last visited order book state is stored so that, in a subsequent step, the match engine will resume the matching from where it stopped last.
If no trades result during the matching process, only the time consumed for the matching process is subtracted from the order.
Otherwise, the sum of the sizes of the trades is subtracted from currently stored inventory in the memory.
Subsequently, the evaluation component calculates the reward based on the completed trades.
If the order is not completely filled after the last step taken by the agent, a market order is executed by the environment in order to enforce the final state and therefore complete the epoch.
Finally, the reward, the next observation state and confirmation of whether or not the order was completely filled (e.g. the epoch is done) is forwarded to the agent.

\subsection{Configuration parameters}
\label{setup:parameters}
For the environment to be sufficiently flexible for agents to place orders in various settings, a total of four configuration parameters have to be defined: \textit{order side (OS), time horizon (H), time step length ($\Delta{t}$)} and \textit{feature type (FT)}.
The $OrderSide$ (previously defined in Eq. \ref{eq:order-side}) specifies whether the orders, which are created within the environment, are intended to be buy or sell orders.
\begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=10cm]{images/setup-time-horizon}
    }
    \caption{Segmented time horizon $H$ with remaining time $t$.}
    \label{fig:setup-time-horizon}
\end{figure}
The time horizon parameter $H$ defines the amount of time allocated in order to fill an order.
The default time horizon is set at 100 seconds, for the reasons described in Section \ref{sec:execution-placement}, and this is the equivalent to the Good-Till-Time option commonly seen in a financial markets (see \ref{sec:ob-characteristics}).
Furthermore, in this environment, we intend the agents to take discrete steps, rather than continuous steps.
The latter would not be applicable in real-world practices where each order has to be submitted to a remote exchange and thereby latency issues occur and API request limitations are applied.
The steps the agents will take are determined by the time horizon in which the agent has to completely fill the order, and therefore, the time horizon is segmented into discrete time steps $t$, as illustrated in Figure \ref{fig:setup-time-horizon}. 
As a result, the number of steps the agent can take are limited to the number of steps $t$.
Each step is of the same length $\Delta{t}$ which, for illustration purposes, has been set to 10 seconds.
We pick $T$ as the maximum value of $t$, indicating that the entire amount of time is remaining, whereas $t=0$ means that the time horizon is consumed.
Consequently, within a single epoch, the GTT of the order is being set to $\Delta{t}$ for each step, until a total of total time of $H$ is reached and the GTT is set to 0.
Lastly, the \textit{feature type} ($FT$) determines which state is to be observed by the agent.
The feature time can either be formed by the private variables of the epoch only - inventory $i$ and remaining time left $t$ - or by a combination of the private variables and one of the features described in Section \ref{sec:feature-engineering}.

\subsection{State}
\label{setup:state}
Unlike in most traditional reinforcement learning environments, each step taken by the agent leads to a complete change of the state space.
Consider a chess board environment, where the state space is the board equipped with figures. 
After every move taken by the agent, the state space would look exactly the same, except for the movements of the figures in that step.
The epoch would continue until the agent either wins or loses the game and the state space would be reset to the very same setup for each epoch.
In the order placement environment however, it is as if, in each step, not only one or two figures of the chess board change their position, but almost all of them.
In addition, a reset of the environment would result in an ever changing setup of the figures on the chessboard.
The reason for this is that the chessboard is, in our case, the order book which is in essence a multivariate, possibly non-stationary, time series which changes over time.
More precisely, the state space $S$ is defined as a sequence of order book states from which an agent can observe an observation state $o_t$ of the historical order book provided, at some point in time in the past.
Therefore, the observation state is the result of the order book state applied to the feature in use: $o_t = FT(OS_t)$.
The final state is reached when the entire inventory is bought or sold, that is $i=0$--Checkmate!.
\\
\\
There are two general types of variables that can be used in order to create an observation state: \textit{private variables} and \textit{market variables} \cite{nevmyvaka2006reinforcement}.
For private variables, the size of the state space depends on the $V$ shares that have to be bought or sold and the given time horizon $H$, resulting in a state $s \in R^2$.
Market variables can be any information derived from the order book at a given moment in time.
In our case, the specified feature type (constructed in Section \ref{sec:feature-engineering}) defines the dimensions of the state the agent observes.
Consequently, market variables increase the state space drastically, due to (1) the initialization of the environment using a random order book state and (2) the dimensionality of the feature set.
Hence, for each step taken by the agent, the order book states are likely to be different and thus the state the agent observes changes equally. 

\subsection{Action}
The action submitted by the reinforcement learning agent defines at which price the place the order.
We define a fixed set of actions that an agent can take and that will correspond to a price level which is relative to the current market price of the state of the historical order book.
Therefore, a discrete action space $A$ is a vector $(a_{min}, . . . , a_{max})$ that represents a range of relative limit levels that an agent can choose from in order to place an order. 
More precisely, $a_{min}$ and $a_{max}$ define how deep and how high the order can be placed in the book.
The action $a \in A$ is an offset relative to the market price $p_{m^T}$ before the order was placed (at time step $t=T$).
Negative limit levels indicate the listing deep in the book and positive listings relate to the level on the opposing side of the book.
Hence, the price of the order placement $p$ at some time step $t$ is $p_t = p_{m^T} + a_i * \Delta{a}$, whereas $\Delta{a}$ is a discrete step size chosen for the actions.
An illustration of this concept is given in Figure \ref{fig:setup-actions}.
\begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=8cm]{images/setup-actions}
    }
    \caption{Actions represent an offset relative to the order price at which to place the order in some order book state $OS_i$ at some time step $t$.}
    \label{fig:setup-actions}
\end{figure}
By default, the action step size $\Delta{a}=\$0.10$.
For example, with $|A|=5$ the total action space is $(p_{m^T}-0.2, p_{m^T}-0.1, p_{m^T}, p_{m^T}+0.1, p_{m^T}+0.2)$.
The action space is configurable and the default implementation is of size $|A|=101$, indicating that $a_{min}=-50$ and $a_{max=50}$ result in order prices of $p=p_{m^T}-\$5$ and $p=p_{m^T}+\$5$ respectively.

\subsection{Reward}
\label{setup:reward}
As described in Section \ref{sec:execution-placement}, the volume-weighted average price (see Eq. \ref{eq:vwap} serves as a measure of the \textit{return} for the order placement.
Consequently, the \textit{reward} is defined as the difference between the market price before the order was placed $p_{m^T}$ and the volume-weighted average price paid or received after the order has been filled. 
Hence, with respect to buying assets, the reward is defined as $r=p_{m^T}-p_{vwap}$ and for selling assets, $r=p_{vwap}-p_{m^T}$.
If no trades result during the matching process, the reward is $r=0$, indicating that no direct negative reward is provided.
The reasons for this are that if the order could not be matched over the course of the given time horizon, when $t=0$, a market order follows which might produce trades at a price worse than the market price before the placement started.
In that case, a negative reward is ultimately given.
As a result, we define the discounted return (Eq. \ref{eq:discounted-return}) as $R_t=\sum_{t'=t}^{t_0}{\gamma^{t'-t}*r_{t'}}$, whereas $t_0$ is the time step at which the agent has its time horizon fully consumed for the order of the current epoch.

Disclaimer: Since we are interested in the general ability of reinforcement learning to learn how to place orders, potential maker or taker fees are not considered in this setup.

\section{Q-Learning agent}
\label{setup:q-learning}
The agent described in this section is generally known as \textit{Q-Learning}\cite{watkins1992q}.
In this work, Q-Learning serves to (1) optimize order placement by using private variables only and (2) to have a measure of comparison while evaluating possible advantages of featuring raw market data by using a Deep Q-Network agent (see Section \ref{setup:dqn} below), which is an extension of the Q-Learning agent.
%However, due to the limitations of Q-Learning, which are described below, this agent cannot make direct use of the features derived in Chapter \ref{chap:data}.
%Therefore, this Section first describes the observation state that is being used by the Q-Learning agent.
%Subsequently, the Q-Learning algorithm is introduced including its use of the Bellman equation.
The name "Q-Learning" refers to the application of the Q-function previously presented (Eq. \ref{eq:q-function}).
More specifically, it relies on the \textit{action-value function} (Eq. \ref{eq:optimal-action-value-function}) that obeys an important identity known as the \textit{Bellman equation}.
The intuition is that: if the optimal value action-value $Q^*(s',a')$ of the state $s'$ at the next time step $t+1$ was known for all possible actions $a'$, the optimal strategy is to select the action $a'$ which maximizes the expected value of $r+\gamma*Q^*(s',a')$,
\begin{equation}\label{eq:bellman}
Q^*(s,a)=\mathbb{E}[r+\gamma \max_{a'} Q^*(s',a')] \ \forall{s}\in{s},  a\in{A},
\end{equation}
whereas $0 \le \gamma \le 1$ is the discount rate which determines the value of future rewards, compared to the value of the immediate reward.
The aim of the iterative value approach is to estimate the action-value function by using the Bellman equation as an iterative update,
\begin{equation}
Q_{i+1}(s,a)=\mathbb{E}[r+\gamma \max_{a'}Q_{i}(s',a')]
\end{equation}
Value iteration algorithms then converge to the \textit{optimal action-value} function $Q_{i} \rightarrow Q^*$ as $i \rightarrow \infty$. \cite{sutton1998reinforcement}
\\
\\
Q-Learning makes use of the aforementioned Bellman equation (Eq. \ref{eq:bellman}), which undergoes an iterative update.
The algorithm has proven to be an efficient and effective choice for solving problems in a discrete state space.
The limitations of this approach emerge when the agent is applied to large or continuous state spaces \cite{gaskett2002q}.
They become more apparent when considering the algorithm presented above.
The iterative update of the action-value function $Q(s,a)$ (defined in Eq. \ref{eq:optimal-action-value-function} and used in Eq. \ref{eq:bellman}) is exposed to the size of state $s$ and action $a$, and thus if $s$ is too large, the optimal policy $\pi^*(s)$ (defined in Eq. \ref{eq:optimal-policy-s}) is not likely to converge.
As a result, the features derived in Chapter \ref{chap:data} are not applicable for this agent.
However, private variables of the environment, as described in Section \ref{setup:state}, respect the aforementioned limitations.
As a result, the observation the Q-Learning agent will make from the environment is defined by the discrete inventory unit $i$ and time step $t$, that is, $s=(i, t)$.
\begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=10cm]{images/setup-inventory}
    }
    \caption{Inventory of $V$ segmented shares with a remaining inventory $i$.}
    \label{fig:setup-inventory}
\end{figure}
The change in the fractions of the remaining inventory that occurs during matching process would, however, still result in a vast state space.
Therefore, the $V$ shares are divided into discrete inventory units $i$ of size $\Delta{i}$, as illustrated in Figure \ref{fig:setup-inventory}, and allow to approximate the order size when an order is updated.
We pick $I$ as the maximum value for $i$, indicating that the entire inventory remains to be filled.
The order is considered as filled when $i=0$, meaning that no inventory is left.
Given the inventory units and the time steps, the state space remains $s \in R^2$ but becomes much smaller in its size, namely $I \times H$.
In the default setup, a segmentation of 0.01 BTC steps is applied.
For example, if the initial inventory is 1.0 BTC and the order is partially filled with 0.015 BTC during an epoch, the remaining inventory is 0.99 BTC (instead of 0.985) for the next step the agent will take.

\begin{algorithm}
\caption{Q-Learning algorithm}\label{alg:q-learning}
\begin{algorithmic}[1]
\State Initialize $Q(s,a)$ arbitrarily
\For{each episode}
\For{t=0...T}
\For{i=0...I}
\State $s=(i, t)$
\State Choose $a$ from $s$ using $\pi$ derived from $Q$ ($\epsilon$-greedy)
\State Take action $a$, observe $r, s'$
\State $Q(s,a) \gets Q(s,a)+ \alpha[r+ \gamma \max_{a'}Q(s',a')-Q(s,a)]$
\State $s \gets s'$
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
Finally, algorithm \ref{alg:q-learning} describes the Q-Learning algorithm used in this work.
An adaptation was made to a conventional Q-Learning algorithm\cite{sutton1998reinforcement} regarding the order of the steps the agents will follow; this adaptation therefore makes use of the same concept as presented in \cite{nevmyvaka2006reinforcement}.
The agent solves the order placement problem inductively, starting with the episode at state $s=(i,0)$, when $t=0$.
This has the benefit that the environment forces a market order to be transacted at the beginning of an epoch, for each inventory unit $i$ and therefore provides immediate reward (see Section \ref{setup:reward}) to the agent.
The agent then increments $i$ and $t$ independently and the previously-seen reward will serve as the future reward, as the agent increases $i$ and $t$.
As a result, for each epoch, the agent takes $I*T$ steps.
The Q function is updated given the state $s$ and action $a$.
Therefore, the existing estimate of the action-value function is subtracted from the value of the action that is estimated to return the maximum future reward.
In addition, a learning rate $\alpha$ is introduced that specifies to which extent new information should override previously learned information, with weighting $0 \le \alpha \le 1$.
Eventually, the agent completes the episode when every combination of $t$ and $i$ has been visited by the agent, that is in state $(I, T)$.
Hence, the agent has learned each discrete step $i$ and $t$ in the process of buying or selling $V$ within the time horizon $H$.

\section{Deep Q-Network agent}
\label{setup:dqn}
The second agent presented in this section is known as Deep Q-Network\cite{mnih2015human}.
DQN is a deep reinforcement learning method that combines reinforcement learning with a class of artificial neural network known as deep neural networks.
\begin{quote}
Notably, recent advances in deep neural networks, in which several layers of nodes are used to build up progressively more abstract representations of the data, have made it possible for artificial neural networks to learn concepts such as object categories directly from raw sensory data.\cite{mnih2015human}
\end{quote}

\begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=10cm]{images/setup-cnn-output}
    }
    \caption{CNN forward pass outputs scalar number for each possible action when observing a particular state.}
    \label{fig:setup-cnn-output}
\end{figure}
Figure \ref{fig:setup-cnn-output} indicates that this model generates one output for each action $a \in A$, resulting in all possible Q values for a particular observed state that is either derived from feature I (Section \ref{sec:data-feature-1}) or feature II (Section \ref{sec:data-feature-2}).
Furthermore, it is valid to note that this is not a classification problem but a regression instead, as the Q-value output is a scalar number.
\\
\\
We use one particularly successful architecture, the deep convolutional network (CNN)\cite{cnn}, which uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive fields, thereby exploiting the correlations present in trading activity.
This model is in line with the architecture described in \cite{mnih2013playing}, except that the input shape changed for the data that we used.
The input to the neural network is determined by the feature to be used.
That is, for feature I (Section \ref{sec:data-feature-1}): $(m+1) \times 2 \times 2n$, whereas $m$ is the number of order book states plus a window consisting of the inventory $i$ and time left $t$, and $n$ is the number of bid and ask levels.
For feature II (Section \ref{sec:data-feature-2}), that is: $4 \times (n+1)$, whereas $n$ is the number of historical trades appended to the row consisting of inventory $i$ and time left $t$.
The first and second hidden layers convolve filters that have stride 4 and stride 2 with respect to the input and apply a rectifier nonlinearity \cite{jarrett2009best}. 
Thereby, we have to ensure that the depth of the filters is equal to the depth of the input.
For feature I, we will choose a filter of size $(m+1) \times 5 \times 5$ and for feature II of size $1 \times 5 \times 5$.
The final hidden layer is fully connected and consists of 200 rectifier units. 
The output layer is a fully-connected linear layer with a single output for each valid action. 
The number of valid actions is therefore defined by $|A|$, which is 201 in the default setup.
We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN-CNN).

Even though the CNN is the standard architecture for the widely-used DQN agent setup, and has been shown to perform well in various fields\cite{mnih2015human, mnih2013playing}, we will provide one additional architecture to be evaluated,
namely, a simple multilayer perceptron (MLP)\cite{haykin2009neural} with two hidden layers, each of which consists of 200 neurons. These layers are followed by the output layer which generates one output for each valid action.
Therefore, the second architecture processes the samples in the same way as the aforementioned CNN approach, except that no convolution will be applied.
As a result, not only will we have a means of comparison alongside an evaluation of the effectiveness of the state-of-the-art DQN agent setup, but we will also have additional confidence while concluding the effectiveness of deep reinforcement learning, when applied to the context of limit order placement. 
We refer to MLP trained with our approach as Deep Q-Networks (DQN-MLP).
\\
\\
The neural network treats the right-hand side of the above-mentioned Bellman equation (Eq. \ref{eq:bellman}), with weights $\theta$, as a target, that is, $r+\gamma \max_{a'} Q^*(s',a', \theta)$.
We then minimize the mean squared error (MSE) with stochastic gradient descent,
\begin{equation}\label{eq:dqn-mse}
    L(\theta_i)=\mathbb{E}[r+\gamma \max_{a'} Q^*(s',a', \theta_{i-1}) - Q(s,a,\theta_i)]^2
\end{equation}

Algorithm \ref{alg:dqn} shows the complete DQN algorithm.
While the optimal q-value converges for the Q-Learning approach by using a look-up table, the DQN approach makes use of a non-linear function approximator.
However, this can cause the convergence due to (1) correlation between samples and (2) non-stationary targets.
In order to remove correlations we use \textit{experience replay} to build a data set $D$ from the agent's own experiences.
Accordingly, we store the agent's experience $e_t=(s_t, a_t, r_t, s_{t+1})$ at each time-step $t$ in the data set, such that $D_t = {e_1, ..., e_t}$.
During the learning process, Q-learning updates on samples (or mini-batches) of these experiences $(s,a,r,s') \sim U(D)$, which are drawn uniformly at random from the pool of stored samples in $D$.
Hence, we prevent the learner from developing a pattern from the sequential nature of the experiences the agent observes throughout one epoch.
In our case, this might occur during a significant rise or fall in the market price.
In addition, experience replay stores rare experiences for much longer so that the agent can learn them more often.
That is, for example, when massive subsequent buy orders led to a noticeable change in the order book.
Furthermore, in order to deal with non-stationarity, the target network parameters $\delta'$ are only updated with $\delta$ every $C$ steps and otherwise remain unchanged between individual updates.
Lastly, no preprocessing is being done in $\phi(.)$, as this is handled by the environment (see Section \ref{setup:state}).

\begin{algorithm}
\caption{Deep Q-learning with Experience Replay}\label{alg:dqn}
\begin{algorithmic}[1]
\State Initialize replay memory $D$ to capacity $N$
\State Initialize action-value function $Q$ with random weights
\For{episode$=1,M$}
    \State Initialize sequence $s_1={o_{random}}$ with observation and preprocessed sequenced $\phi{}_1=\phi{(s_1)}$
    \For{t=1,T}
        \State With probability $\epsilon$ select a random action $a_t$
        \State otherwise select $a_t=\max{}_aQ^{*}(\phi{(s_t)}, a; \theta)$
        \State Execute action $a_t$ in emulator and observe reward $r_t$ and observation state $o_{t+1}$
        \State Set $s_{t+1}=s_t,a_t,o_{t+1}$ and preprocess $\phi_{t+1}=\phi{(s_{t+1})}$
        \State Store transition $(\phi_t,a_t,r_t,\phi_{t+1})$ in $D$
        \State Sample random minibatch of transitions $(\phi_j,a_j,r_j,\phi_{j+1})$ from $D$
        \State Set
            \[
                y_j = \begin{cases}
                    r_j & \textrm{for terminal}\ \phi_{j+1}\\
                    r_j+\gamma \max_{a'} Q^*(\phi_{j+1},a', \theta) & \textrm{for non-terminal}\ \phi_{j+1}
                \end{cases}
            \]
        \State Perform a gradient descent step on $(y_j-Q(\phi_j,a_j;\theta))^2$ according to Eq. \ref{eq:dqn-mse}
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
