\chapter{General conclusions and discussion}
\label{chap:discussion}

This chapter discusses and revisits the research questions with respect to the corresponding contributions made and thereby mentions strengths and limitations of this work.
Furthermore, a summary of the contributions made is provided.
Finally, future work is suggested and a motivation is stated which supports the application of the techniques developed to be used in real world practices.

\section{Summary of contributions}

This work aims to take a step towards answering the important question of how one can optimize limit order placement on cryptocurrency exchanges using deep reinforcement learning.
Previous work in this field, by Kearns et al., who studied the behavior of order placement and order execution\cite{nevmyvaka2005electronic} and developed a reinforcement learning strategy\cite{nevmyvaka2006reinforcement} for the purposes of optimization on traditional exchanges, has been extended and applied to the cryptocurrency field.

In this work, raw market data of the Bitcoin/US dollar trading pair was analyzed with regard to the activity of market participants.
We have found that patterns emerge while traders create and cancel orders in financial markets.
Based on these findings, two features where designed which incorporate these patterns, in the form of a window of 1) historical order book states and 2) historical trades.
Furthermore, we developed a limit order book data structure and a simplified match engine; the latter emulates a local broker and can match orders using the historical order book.
These mechanisms were incorporated into a reinforcement learning environment which enables the order placement problem to be transposed into the reinforcement learning context.
This environment is configurable and therefore flexible enough to enable investigations by means of the previously constructed features as well as a variety of agents.
In addition, two implementations of reinforcement learning agents were developed: a Q-Learning agent, which serves as the learner when no market variables are provided, and a Deep Q-Network agent which was developed to handle the features previously mentioned.

Prior to the experiments with the environment and using the agents developed, a comprehensive evaluation procedure was elaborated.
This multi-step procedure involved an empirical investigation of the expected costs of limit and market orders and serves as a benchmark for the Q-Learning agent as well as the DQN agent.
At the same time, the empirical investigation allowed us to improve the parameter setting of the reinforcement learning environment prior the more costly experiments that involved agents.
The results of a backtest have shown that the Q-Learning agent was not able to take advantage of market price movements that were favorable to the order placement process.
However, the Q-Learner was able to reduce the costs when the market price development was not favorable to either buying or selling assets.
The DQN agent was backtested under the application of two different neural network architectures as well as two aforementioned feature settings.
Thereby, we have found that the DQN-CNN agent (convolutional neural network), that is widely used in many fields of literature, is superior to the DQN-MLP agent (multilayer perceptron).
Furthermore, we have achieved significantly better performance by using feature I (historical trades) compared to feature (II) historical order book states.
Our experiments have shown that a feature vector containing the 30 most recent historical trades performed best among a range of settings tested.
The final outcome was that the policy learned was able to optimize order placement when market conditions were favorable to making a purchase or sale.
In addition, when conditions were not ideal, the agent still performed better than the expected costs of a market order, improving the purchase and sale of 1.0 Bitcoin by \$33.89.
Moreover, we observed the decision-making process of the agent and found further limitations in our setup, which suggest that, with further tuning, our results could improve.
Furthermore, it has been shown that the DQN agent is capable of finding a near-optimal policy on artificially generated market data.

Our work confirms that there is indeed a way in which deep reinforcement learning is capable of optimizing the placement of limit orders.
However, under the application of real world market data, the agent can be exposed to severe conditions in the form of wide spreads or an absence of liquidity, both of which prevent the agent from applying an optimal policy.
Therefore, and in order to constantly be able to perform placements that are better than or equal to what is offered on financial markets at any given time, the approaches presented in this work have to be extended.

\section{Findings with regard to the research questions}

In Chapter \ref{chap:introduction} (Section \ref{sec:intro-rq}), the purpose of this thesis and the research objectives were stated.
This section aims to revisit these research questions, conclude what has been achieved in respect of each of them, and then summarize and discuss the findings in greater detail.

\subsection{RQ 1.1: Which historical market data patterns drive market participants to buy or sell assets, and how can these patterns be incorporated into features used by a deep reinforcement learning agent?}

    Historical market event data was investigated in Chapter \ref{chap:data}. 
    Thereby, patterns were found which give insight into how market participants positioned their orders, with respect to price and size. 
    It was shown that the price movements were likely to be due to (1) an imbalance between bid and ask orders, (2) a distinctive way of posting or canceling orders, and (3) consecutive or impulsive trades.
    Furthermore, hypotheses were formed based on these findings and two features were constructed that incorporate emerging data containing the patterns found.
    Namely, a window of historical limit order book states (feature I) and historical trade events (feature II).
    \\
    \\
The first attempt to find patterns in market data was made by investigating the relationship between trade activity (volume) as well as price change and volatility, as proposed by Karpoff et al. \cite{karpoff1987relation}.  
As traders submit limit orders to buy (sell), they impact the bid (ask) volumes of the  limit order book. This gives us a view of tradersâ€™ intentions and enables us to foresee the direction of the upcoming price changes.
To quantify these intentions, we looked at the differences between the bid and ask volume, called order imbalance, as shown in Section \ref{sec:data-hypthesis-order-volume} (Table \ref{tbl:data-imbalance}).
Even though some imbalance was clearly present, the high volatility in the Bitcoin/USD market prevented us from finding obvious an correlation with the direction of future price movements.
This gave reasons to believe that the data had to be investigated at the raw market event level\cite{kane2011analyzing}.
The visualization on a volume map of market events occurring over time enabled the detection and association of patterns to future price changes (Section \ref{sec:data-hypthesis-order-trade-volume-time}).
It was found that orders submitted and canceled, as well as the trades generated, had an impact on the future price movement.
More precisely, Figure \ref{fig:data-volmap-crated} and \ref{fig:data-volmap-cancelled} showed examples of a patterns that we have classified, with references to \cite{schluetter2010system, biais1995empirical} as buy- or sell-walls and consecutive posting- or cancelling.
Moreover, consecutive or impulsive trades occurred at the market were detected in Figure \ref{fig:data-volmap-traded} and their impact on the market price was shown in Figure \ref{fig:data-trade-volume}.
However, in order to determine whether or not this information improves the optimization of limit order placement, a feature had to be constructed that enables an agent to attempt to learn a policy.
Hand-crafting the features would have mean to measuring the change in volume and price with for each order and trade event with respect to the previous event occurred at the market, and thereby ensure to include important information such as the frequency at which the orders and trades were posted.
Instead, we have chosen an approach that was demonstrated to be successful in other fields.
Recent advances in deep reinforcement learning have demonstrated the ability to learn directly from pixel frames\cite{mnih2013playing} and raw sensory data\cite{mnih2015human} occurring over time.
We followed the same principles and constructed features which reflect (1) the order book states and (2) the trades that occurred over time.
Feature I, as described in Section \ref{sec:data-feature-1}, is a vector that contains the most recent order book state and reflects the historical evolution of this state with a window $m$.
Feature II, as described in Section \ref{sec:data-feature-2}, is a vector that lists the $n$ most recent historical trades including the frequency of their occurrence.
As a result, the features constructed will contain new information as soon as a new market event is published; at that point, a new sample can be added to the feature set without the need for a preprocessing step.
One of the difficulties with the features constructed was determining the appropriate amount of historical data to be considered.
Attempts were made by measuring the entropy and correlation of order prices and sizes but ultimately, this did not give clear indications of how many historical order book states or trades should be considered.
Subsequently, we evaluated the various feature settings during the learning process (see Section \ref{subsec:eval-comparison-feature-size}) and were able to find the desired settings.
It was shown that for feature I the window size $m=30$ performed best among the sizes 10, 20, 30, 40, and 50.
Likewise, improvements with respect to the outcome of the placed orders could be achieved by considering up to $n=30$ historical trades with feature II.

\subsection{RQ 1.2: How can one design a reinforcement learning environment and agents, in the context of order placement?}

    The challenging task of mapping the order placement problem into the reinforcement learning context demanded an investigation of components involved in the financial setting as well as in the reinforcement learning setting; this was described in Chapter \ref{chap:preliminaries}.
    In Chapter \ref{chap:setup}, these components were combined and a reinforcement learning environment was built (see Figure \ref{fig:rl-env-overview}).
    This environment configures a discrete action space that enables an agent to explore the state space that is determined by the underlying historical market data.
    The reward function used represents the outcome of the matching process of an order submitted.
    We were able to assign the most complex tasks to the environment and thereby drastically reduce the level of complexity of the agents.
    Two agents with the purposes of learning with and without market features were developed and will be discussed below.
    \begin{figure}[H]
    \centering
    \makebox[\linewidth]{
        \includegraphics[width=12cm]{images/discuss-rl-sv.png}
    }
    \caption{Illustration of suitability of reinforcement learning and supervised learning.}
    \label{fig:discuss-rl-sv}
    \end{figure}
    We will first reflect on why reinforcement learning is an appropriate approach to be taken in order to optimize the limit order placement problem.
    Literature reports the uses of supervised learning methods (Section \ref{sec:related-supervised}) to forecast price movements and suggests the application of limit order placement as a separate task.
    We describe the reason for the separation of these tasks with the supporting Figure \ref{fig:discuss-rl-sv} above.
    The outcome of an order placement is always determined by the match engine that makes use of the underlying market data.
    In our case that is a local setup with historical market data; when applied in a real-world setup the match engine would be handled by the exchange where the live market data is determined by the ongoing trading activities.
    Therefore, multiple issues arise with a supervised learning approach.
    First of all, in order to respond to changing market conditions, the placement of a limit order involves sequential decisions making (cancelling and re-submitting the order). 
    In supervised learning this implies to predict a sequence of actions (prices at which to place the order) by a classifier \cite{sequence-prediction}.
    In our context, two problems would arise with such an approach:
    a match engine (or an exchange in the real world setup) is always laid out to process only one order at a time and therefore an additional scheduling mechanism would be required to handle the submission of a sequence of orders.
    The second difficulty is related to the fact that the rewards (labels) are not known prior the submission of an order.
    Therefore, one would have to generate a sample data set by first precomputing a number of sequences of actions, and second, processing their outcome using the match engine.
    This process is not only computationally expensive but would also require a method to reduce the vast number of combinations possible.
    In contrast, with reinforcement learning, it is possible to model an end-to-end learning process that is known to be successful in other domains\cite{amodei2016deep, mnih2015human, mnih2013playing}.
    As shown in the Figure above, this process becomes not only simpler but also uses the given functionality of a match engine and therefore makes it applicable in a real world environment.
    The steps involved are as follows: one order is submitted at a time by the agent.
    The order is then updated according to the action chosen as well as the time and inventory remaining, with each step taken.
    As the agent proceeds with its exploration and exploitation, it is able to reduce the combinations of actions to be taken by a great extent and learns a policy (i.e. an order placement strategy) by receiving immediate and future rewards for each step taken.
    
    A successful application of reinforcement learning in the context of limit order placement is known in literature\cite{nevmyvaka2006reinforcement} and was introduced earlier, in Section \ref{sec:related-reinforcement}.
    While one of the goals of our work was to make use of raw market data, the authors' approach made use of hand-crafted features;  therefore, the only solution available to us was to use an action-value function approximator (see Section \ref{sec:deep-reinforcement-learning}).
    This in turn demanded the application of our own reinforcement learning framework.
    \\
    \\
    Many difficulties arose during the construction of the reinforcement learning environment and important findings were made.
    The order book (Section \ref{sec:order-book}) and the match engine (Section \ref{sec:match-engine}) proved to be the key components which ultimately enabled a local broker to be emulated and therefore also to facilitate the simulation of limit order placement using historical order books.
    The difficulties that arose during the development of these components were related to performance and the inevitable approximations required during the matching process.
    The vast number of market events led the matching process to be become unbearably slow.
    Improvements were made by using caching and indexing mechanisms but slow speed remains a limitation of this project. The run-time for an agent to train over 5000 epochs took more than 4 hours.
    Ultimately, this limited the scope of the evaluation that was undertaken. 
    Undoubtedly, improvements could be made by using techniques such as those proposed in \cite{barazzutti2016exploiting}.
    That is, exploiting parallel processing capabilities of modern multi-core CPUs when constructing a limit order book data structure.
    However, this work was beyond the scope of this project as the focus was placed on the general principle of how to map the order placement problem into the reinforcement learning context, rather than building an efficient matching engine.
    A more serious limitation when matching orders using historical order books is the absence of market participants, who could have 1) entered or 2) left the market upon placing an order during the simulation.
    Participants who would enter the market would probably be favorable to the order submitted as they would act as potential buyers and sellers and therefore provide liquidity.
    Participants who leave the market would introduce a slight disadvantage as there would be less liquidity.
    Therefore, the matching process is strictly speaking an estimation of what would have happened in the past.
    However, since only up to 1.0 BTC were used for buying and selling, which is insignificant in terms of market impact\cite{hautsch2012market}, we considered the approximation of the matching process as sufficiently correct.
    Otherwise, the only way to overcome this limitation would have been to train with and test on live market data and make real purchases and sales.
    
    Due to the complexity of the limit order placement process it was necessary to equip the environment with a variety of configuration parameters, as defined in Section \ref{setup:parameters}.
    While this provides the ability to define evaluation setups very precisely, it also introduces the inevitable obligation of limiting the scope of our approach.
    We have decided to define a discrete time step size $\Delta{t}=10s$, which represents the duration of how long an order remains in the order book for each step taken by the agent and therefore limits the number of steps to be taken throughout one epoch to $\frac{H}{\Delta{t}}=10$.
    With an empirical investigation (Section \ref{sec:eval-empirical}), we have shown that a placement of 10 seconds diverges from the expected rewards received from placing the order for a total of 100 seconds.
    Naturally, if the agent decided in each step to choose the same action, the result would be equivalent to a placement over 100 seconds.
    Therefore, the benefit of the time-step parameter is the agent's ability to intercept and change the action accordingly, in the event of a sudden change in the market data.
    Investigations were made with time-step parameters smaller than 10 seconds (3s, 5s) and no improvements could be seen.
    The only difference was an increased training time as the agent had to take more steps in every epoch.
    Another important parameter is the size of the action space, which we defined as consisting of 50 negative and 50 positive actions that result in a total action space size of $|A|=101$.
    The empirical investigation in Section \ref{sec:eval-empirical} has confirmed that this is indeed the range of actions which have most influence in the posting of orders over a time horizon of 100 seconds.
    However, the limitations found in Section \ref{sec:eval-dqn-limitations} showed that there are rare occasions where the defined action space is too small and therefore the agent is unable to place the order at a price level close to the offers made by potential buyers or sellers.
    Increasing the already- large action space would certainly allow the agent to place orders at the appropriate price levels.
    However, experiments with $|A|=201$ have shown that this comes with the cost of increased training as well as a greater potential for misplacing orders; all these factors meant that performance could not be improved.
    In addition, different settings for the action step size $\Delta{a}$, which would enable us to increase the price range at which orders are placed by maintaining equal action space size.
    However, this implies that the agent would make more coarse decisions in terms of the price level at which to place orders and therefore potentially optimal placements would be out of reach.
    Our investigations have shown (Table \ref{tbl:analysis-empirical-summary-a}) that no improvements could be made by increasing the action steps and the optimal setting is $\Delta{a}=\$0.10$.
    \\
    \\
    The use of the OpenAI Gym toolkit\cite{brockman2016openai} allowed us to design our environment in a way such that multiple agent could make use of the environment.
    The Q-Learning and DQN agents developed are similar in terms of the reward function as both underlie the principles of the Bellman equation (Eq. \ref{eq:bellman}).
    However, much complexity is added to the DQN agent, such as the use of an \textit{experience replay} and most importantly a \textit{value function approximator}.
    The neural network used served to approximate the patterns within our data set that were found and described earlier.
    This enabled us to feed private and market variables to this agent.
    Furthermore, this agent provided great flexibility when constructing the feature vectors.
    In contrast, the Q-Learning agent was not found to be a useful choice in this regard.
    While the Q-Learning agent can be suitable when no market variables or approximations thereof are applied, it was found not to be an appropriate choice for the purpose of optimizing the limit order placement problem.
    Market features such as the ones constructed in Chapter \ref{chap:data} did not provide any benefit when applied to the Q-Learning agent.
    The reason for this is that Q-Learning makes use of a look-up table which records a combination of all observed states and chosen actions.
    Since the observation states are derived from market data at a given point in time, most of them are unique and thus learning time is expected to scale exponentially with the size of the state space\cite{whitehead1991complexity, kaelbling1996reinforcement}.
    To overcome this limitation, market features would have to be approximated drastically, as demonstrated in \cite{nevmyvaka2006reinforcement}.
    
    Furthermore, we briefly investigated the architecture of the neural network and ultimately equipped the DQN agent with the widely-used convolutional neural network architecture\cite{mnih2015human}, defined as DQN-CNN, as well as with a simpler multilayer perceptron with two hidden layers, defined as DQN-MLP.
    Our investigations (Section \ref{sec:eval-dqn}) showed that the results were similar and the DQN-CNN setup performed best among all tested.
    We found that having a second network architecture helpful as it provided supporting numbers within our results and further confirmed the effectiveness of deep reinforcement learning applied in our context.
    However, there is room for improvements with regard to parameter tuning, which we neglected in this work in favor of prioritizing the architectural aspects of the reinforcement learning setup; we therefore suggest this as future work.

\subsection{RQ 1.3: How can one evaluate a reinforcement learning environment and agent in the context of order placement?}

    No previous research has yet been applied to the cryptocurrency market and therefore there are no optimization performance numbers, to which the approach of this work can be compared. 
    As a result, an evaluation procedure was elaborated as part of this thesis (Section \ref{sec:analysis-procedure}).
    Therefore, two real-world market data sets were chosen as well as artificially generated limit order books.
    This multi-step procedure involved an empirical investigation of the expected costs of an optimally placed limit order as well as the costs of an immediate purchase or sale using a market order.
    Furthermore, the expected costs were taken as a benchmark for the Q-Learning agent as well as the DQN agent under the application of both aforementioned feature types.
    In addition, an evaluation mechanism was built into the reinforcement learning environment which enabled the investigation of the steps taken by an agent while running a training or testing epoch.
    \\
    \\
    The way in which order placement was simulated relied significantly on suggestions related to backtesting made by Marcos Lopez de Prado \cite{de2018advances}.
    The selection of a random order book state that serves as the starting point of an epoch drastically reduces the chances for possible over-fitting.
    With this in mind, we considered data sets which show different market price constellations and furthermore, developed a way to create artificial order books on which to train and test the agent.
    It is obvious that, only by considering an infinite amount of data sets can a completely generalized statement about the optimization capabilities be made; however, as mentioned earlier, this comes at the expense of time and computational resources.
    However, our novel approach to generating artificial data sets has proven to be of substantial value since the possible optimization factor is known prior to the validation of the model and processing is much faster.
    
    The empirical investigation step executed prior to the training of a policy has provided baseline performance results.
    With this approach we were able to determine and improve the parameter setting of the reinforcement learning environment, without proceeding actual learning tasks, which made it very efficient.
    Furthermore, we were able to determine the expected costs for market and limit orders and the results have shown that the expected behavior for limit order placement is similar to what was proposed by Kearns et al. \cite{nevmyvaka2005electronic}, who worked with traditional stocks.
    However, we have given more details of this behavior with respect to the given time horizon for the placements of orders.
    The authors of \cite{nevmyvaka2005electronic} stated that, overall, the optimal price at which to place an order is just below the spread price.
    This statement is true for the cryptocurrency market as well, however, only on condition that the time horizon chosen is either very small ($H\leq10s$) or large ($H>100s$).
    Otherwise, for example when $H=100s$, we have demonstrated that the optimal price at which to place an order changes drastically depending on the fluctuations of the market price.
    Namely, when buying, limit orders should be placed deep in the order book when market prices are falling, and high in the book when the price is rising.
    On the contrary, when selling, orders should be place high in the book when the market price is falling and low in the book when the price is rising.
    These insights not only provide knowledge of how to optimize the placement of orders but also generated the means to compare the performance achieved by reinforcement learning agents.
    
    We further reflect that average rewards provide a much greater insight into the performance of reinforcement learners, as opposed to the average actions chosen.
    An average action, consisting of up to 10 values (equivalent to the maximum number of steps for one epoch) from a range of 101 total actions, certainly provides understanding of the average price levels at which the learners attempt to place orders.
    However, it is suggested that an observation and analysis of the specific sequence of actions chosen be undertaken in future work.
    
    Another important method introduced during the evaluation was the possibility to record and visualize the decision-making process of the reinforcement learning agents, as shown in Section \ref{sec:eval-dqn-limitations}.
    Thereby we were able to observe in which market situations the agents did not respond with the most appropriate actions.
    This detected the limitations of the agent as well as the environment in greater detail.
    
    Lastly, by considering the artificial data sets with linear and sine functions applied, near-optimal performance was achieved with the DQN agent and feature type I (window of historical order book states).
    This confirms that 1) reinforcement learning is indeed capable of optimizing limit order placement but 2) that there is potential to further improve the setup for real-world market data sets.
    Moreover, the increased optimization performance using a sine-shaped order book, compared to real-world market data, indicates that the learner had much difficulty finding a policy under non-stationary conditions and the $C$ parameter (see Section \ref{setup:dqn}) was only of minor assistance.
    
\subsection{RQ 1.4: In which way do the constructed features enable a reinforcement learning agent to improve the way it places orders?}

    The aforementioned evaluation procedure was used to determine the efficiency of the constructed features that were applied to the DQN agent.
    The results given in Chapter \ref{chap:analysis} showed that, with the use of either of the two features, a policy can be learned that enables the  optimization of order placement when market conditions are favorable to making a purchase or sale.
    However, under the application of either of the two features, the agent was not able to performed significantly better than the expected costs of a market order when conditions were not ideal.
    Moreover, it has been shown that the application of feature II (a sequence of historical trades) results in a better policy overall than the application of feature I (a window of historical order book states).
    \\
    \\
    First, reinforcement learning without market features was tested by using the adapted Q-learning algorithm presented in Section \ref{setup:q-learning}.
    The similar approach presented by Kearns et al. \cite{nevmyvaka2006reinforcement}, and tested on traditional stocks, achieved an optimization in the range of 27.16\% to 35.5\% for sell limit orders placed.
    We backtested both buying and selling scenarios on the cryptocurrency market Bitcoin/USD.
    Interestingly, we achieved a similar performance with the Q-Learner, when the market price moved against our favor while attempting to make a sale.
    That is, \$-27.70 was the expected return for selling 1.0 BTC and the agent reduced the return to \$-21.34, e.g. an improvement of 22.96\%.
    However, when the market price was rising, which is in favor of selling, the agent was unable to outperform the expected market order costs of \$-1.72 and a loss of \$-4.74 was generated.
    In the buying process, the return improved from \$-1.06 expected, to \$-1.04, when the market price was rising; no improvements were achieved when the price moved against our favor--\$-1.17 compared to \$-0.05 expected.
    In general, it can be said that the strategy learned by the Q-learner is relatively close to the costs of a market order, with one exception where it was significantly more profitable.
    That suggests that the absence of market variables (our constructed features) allows the agent to learn the general principles of determining the price level at which to submit orders. 
    Nevertheless, the agent was still not able to perform better than a market order.
    
    The results found for our best setup of the DQN agent are an improvement over ones for the Q-Learning agent.
    During the empirical investigation, we stated that the sum of the expected market order returns was \$-30.53 and the sum of optimally placed orders was \$-9.88.
    The agent under the application of feature I (historical order book states) achieved a total reward of \$-18.62 and, under the application of feature II, a total reward of \$3.36.
    This was surprising since the structure of feature I is much more like the one used in \cite{mnih2013playing} and better results were expected therefrom.
    However, this suggests that the information provided by historical trades is much more beneficial than information from historical order book states.
    Furthermore, we found that the DQN agent had the most difficulties detecting changes in the market price trend, as shown in Section \ref{sec:eval-dqn-limitations} (Figure \ref{fig:analysis-limit-impatient}).
    Thereby the agent expected rewards according to the input sample provided and therefore did not consider a possible reversal of the trend.
    However, during the evaluation of the limitation of the DQN agent (Section \ref{sec:eval-dqn-limitations}), we found no improvements to its performance when enlarging the window size of the feature provided to the agent and thereby covering long-term market movements.
    In addition, we increased the number of training epochs to 25,000 in the hope that the agent could improve its selection of actions in difficult market conditions, however, no improvements were found.
    At this point it is to be assumed that increasing the data sets and, at the same time, enlarging the feature vectors may improve the performance.
    
    The next step taken involved the use of the artificial order books generated to which we applied the same feature vector setup as in the experiments undertaken with historical market data.
    Thereby, we found that the DQN agent was capable of finding a near-optimal policy for an order book that underlies a linear function as well as one that underlies a sine function.
    This suggests that the features consisting of non-stationary as well as noisy market data prevented the agent from achieving similar results.

\section{Recommendations and future work}

The most significant recommendation would be to run simulations of limit order placement with the setup provided in this work on a live market where it would make actual sales and purchases.
Thereby, it would be possible to determine to what extent market participants that enter or leave the market, during the placement of an order, would affect the conclusions found in this work.
By doing so, market fees could be considered and integrated into the reward function provided. 
This process could then be followed by an observation; the purpose of which would be to determine whether or not the agent adjusts its actions such that market orders that come with the more expensive \textit{taker fees} are chosen less often.

An alternative to a live market evaluation would be to make use of an entire artificial market, such as the one proposed by Raberto et al. \cite{raberto2005price}. 
Such a setup would require multiple agents to continuously post buy and sell orders.
In this process, some of the agents could act as naive traders and others could make use of the learning capabilities provided in this work.
It would then be interesting to see which category of agents would achieve a better performance overall.

Furthermore, some limitations were discovered in this work and further investigations were therefore suggested.
This includes the extension of the evaluation procedure in which the sequence of actions chosen by an agent is investigated.
The aim of this is to understand what makes an agent choose for a certain action over another.
Additionally, the model presented and used in the DQN agent provides hyper-parameters which are to be further optimized.
A systematic grid search would therefore provide insights into whether or not the problem can be further optimized.

In addition, this work could be extended by taking a hybrid approach with the help of imitation learning.
A statistical framework such as the one proposed in \cite{yingsaeree2012algorithmic} (see Section \ref{sec:related-statistical}) would act as the expert agent from which an initial policy is learned.
Subsequently, our deep reinforcement learning mechanisms could be used to learn patterns from the market data that is favorable to limit order placement.
Thereby, we suggest updating the policy only if the rewards are either significantly positive or negative, in order to prevent  noise being learned.

Lastly, the setup built in this work can be used not only to learn order placement but also to learn a specific market making\cite{o1986microeconomics} strategy.
This strategy is to place buy and sell orders simultaneously with the incentive of making a profit from the difference between the price paid for the buy order and the price received for the sell order.

\section{Application in real world practices}

This work involved a pipeline of tasks, each of which contributed in part towards answering the research questions stated in this thesis.
The tools and components developed throughout this process were consecutively extended and merged.
The result of this is a ready-to-use product, in the form of a flexible framework that can implement an intelligent order placement strategy according to the historical or live market data provided by the end user.
The configuration parameters of the environment and agent provided enable further adjustments to be made according to the user's specific needs.
Moreover, our approach is not limited to cryptocurrency markets but also supports any product that is traded with limit order books, as long as data are provided.
This in turn makes our setup attractive for numerous real-world settings.
Financial exchanges could use this framework in order to provide a new \textit{order type} that allows their customers to buy or sell an inventory $I$ of an asset within a time horizon of $H$ seconds.
Moreover, this setup is not restricted to centralized exchanges but can also be applied to decentralized ones where each node of the exchange would employ its independent agent that learns from the corresponding order book residing in that node.
Another target group for this framework is (institutional) traders or brokers which intend to optimize the way in which they place orders on the exchanges of their choice.
In this case, exchange data serves as the source of the reinforcement learning environment and the agent's task is to act as an \textit{intermediary} between the trader and exchange.